{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "    dev = \"cuda:0\" \n",
    "else:  \n",
    "    dev = \"cpu\"  \n",
    "device = torch.device(dev)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 24\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "log_interval = 10\n",
    "\n",
    "num_vectors = 4\n",
    "len_vectors = 10\n",
    "img_height = 28\n",
    "img_width = 28\n",
    "batch_size = batch_size_train\n",
    "win_size = 3\n",
    "epsilon = .7\n",
    "epochs = 500\n",
    "steps = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_state(example_data):\n",
    "    temp_example_data = torch.reshape(example_data,(batch_size,img_height,img_width))\n",
    "    temp_inp = [temp_example_data for i in range(10)]\n",
    "    temp_inp_data = torch.stack((temp_inp),dim = 3)\n",
    "    return temp_inp_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def targets_to_state(example_targets):\n",
    "    temp_out_state = torch.nn.functional.one_hot(example_targets,num_classes=10).repeat(1,img_height*img_width)\n",
    "    temp_out_state = temp_out_state.view((batch_size,img_height,img_width,10))\n",
    "    return temp_out_state.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CA Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_state(batch_size,img_height,img_width,num_vectors,len_vectors):\n",
    "    state = torch.rand((batch_size,img_height,img_width,num_vectors,len_vectors))*.1\n",
    "    return state.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self, num_inp,num_out):\n",
    "        super(model, self).__init__()\n",
    "        self.Q1 = nn.Linear(num_inp, num_out)\n",
    "        self.K1 = nn.Linear(num_inp, num_out)\n",
    "        self.V1 = nn.Linear(num_inp, num_out)\n",
    "        \n",
    "        self.act = nn.LeakyReLU()\n",
    "        self.act1 = nn.Sigmoid()\n",
    "    def forward(self,x):\n",
    "        \n",
    "        Q = self.act(self.Q1(x))\n",
    "        K = self.act(self.K1(x))\n",
    "        V = self.act1(self.V1(x))\n",
    "        \n",
    "        relevance = F.softmax((Q*K),dim=1)\n",
    "        out = relevance*V\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all(bottom_up_model_list,top_down_model_list,layer_att_model_list,state,len_vectors,num_vectors,batch_size):\n",
    "    #shift state to in 9 directions along the x and y plane\n",
    "    roll1 = torch.roll(state, shifts=(-1,-1), dims=(1,2)).to(device)\n",
    "    roll2 = torch.roll(state, shifts=(-1,0), dims=(1,2)).to(device)\n",
    "    roll3 = torch.roll(state, shifts=(-1,1), dims=(1,2)).to(device)\n",
    "    roll4 = torch.roll(state, shifts=(0,-1), dims=(1,2)).to(device)\n",
    "    roll5 = torch.roll(state, shifts=(0,0), dims=(1,2)).to(device)\n",
    "    roll6 = torch.roll(state, shifts=(0,1), dims=(1,2)).to(device)\n",
    "    roll7 = torch.roll(state, shifts=(1,-1), dims=(1,2)).to(device)\n",
    "    roll8 = torch.roll(state, shifts=(1,0), dims=(1,2)).to(device)\n",
    "    roll9 = torch.roll(state, shifts=(1,1), dims=(1,2)).to(device)\n",
    "    roll_list = [roll1,roll2,roll3,roll4,roll5,roll6,roll7,roll8,roll9]\n",
    "    \n",
    "    #concatenate vectors so that att_list contains the state and every adjacent vector on the same vector level\n",
    "    att_list = torch.cat(roll_list,dim=4)\n",
    "    \n",
    "    #feed layers to models:\n",
    "    #top-down models don't get first two layers as input and don't add to 1st & last layer\n",
    "    #bot-up models don't get last layer as input and don't add to first layer \n",
    "    #adjacent models don't add to first layer\n",
    "    delta = [torch.zeros((batch_size*img_height*img_width,len_vectors)).to(device) for i in range(num_vectors)]\n",
    "    for i in range(num_vectors):\n",
    "        if(i<num_vectors-2):\n",
    "            top_down_temp = top_down_model_list[i](torch.reshape(att_list[:,:,:,i+2,:],(-1,len_vectors*9)))\n",
    "            delta[i+1] = delta[i+1] + top_down_temp\n",
    "        if(i<num_vectors-1):\n",
    "            bottom_up_temp = bottom_up_model_list[i](torch.reshape(att_list[:,:,:,i,:],(-1,len_vectors*9)))\n",
    "            att_layer_temp = layer_att_model_list[i](torch.reshape(att_list[:,:,:,i+1,:],(-1,len_vectors*9)))\n",
    "            delta[i+1] = delta[i+1] + bottom_up_temp + att_layer_temp\n",
    "    \n",
    "    #format delta so that delta and state can be added together\n",
    "    delta = torch.stack(delta,dim=1)\n",
    "    delta = torch.reshape(delta,(batch_size,img_height,img_width,num_vectors,len_vectors))\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_up_model_list = [model(9*len_vectors,len_vectors).cuda() for i in range(num_vectors-1)]\n",
    "top_down_model_list= [model(9*len_vectors,len_vectors).cuda() for i in range(num_vectors-2)]\n",
    "layer_att_model_list = [model(9*len_vectors,len_vectors).cuda() for i in range(num_vectors-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list = []\n",
    "for i in range(num_vectors):\n",
    "    if(i<num_vectors-2):\n",
    "        param_list =  param_list + list(top_down_model_list[i].parameters())\n",
    "    if(i<num_vectors-1):\n",
    "        param_list =  param_list + list(bottom_up_model_list[i].parameters())\n",
    "        param_list =  param_list + list(layer_att_model_list[i].parameters())\n",
    "        \n",
    "optimizer = torch.optim.Adadelta(param_list, lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)\n",
    "mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(train_loader)\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #get batch of images\n",
    "    batch_idx, (example_data, example_targets) = next(examples)\n",
    "    \n",
    "    #initialize state\n",
    "    state = init_state(batch_size,img_height,img_width,num_vectors,len_vectors)\n",
    "    \n",
    "    #put current batches into state\n",
    "    state[:,:,:,0,:] = data_to_state(example_data)\n",
    "    for step in range(steps):\n",
    "\n",
    "        delta = compute_all(bottom_up_model_list,top_down_model_list,layer_att_model_list,state,len_vectors,num_vectors,batch_size)\n",
    "        \n",
    "        state = state + delta\n",
    "        \n",
    "    #get loss\n",
    "    pred_out = state[:,:,:,-1]\n",
    "    targ_out = targets_to_state(example_targets).to(device)\n",
    "    loss = mse(pred_out,targ_out)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"Epoch: {}/{}  Loss: {}\".format(epoch,epochs,loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_corr = 0\n",
    "tot_batches = 0\n",
    "for data, target in test_loader:\n",
    "    tot_batches+=batch_size\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #get batch of images\n",
    "    batch_idx, (data, target) = next(examples)\n",
    "    \n",
    "    #initialize state\n",
    "    state = init_state(batch_size,img_height,img_width,num_vectors,len_vectors)\n",
    "    \n",
    "    #put current batches into state\n",
    "    state[:,:,:,0,:] = data_to_state(example_data)\n",
    "    state = state.to(device)\n",
    "    for step in range(steps):\n",
    "        delta = compute_all(bottom_up_model_list,top_down_model_list,layer_att_model_list,state,len_vectors,num_vectors,batch_size)\n",
    "        \n",
    "        #update state\n",
    "        state = state + delta\n",
    "        \n",
    "    for batch in range(batch_size):\n",
    "        temp = torch.zeros((10))\n",
    "        for height in range(img_height):\n",
    "            for width in range(img_width):\n",
    "                ind = torch.argmax(state[batch,height,width,-1])\n",
    "                temp[ind]+=1\n",
    "        \n",
    "        if(target[batch] == torch.argmax(temp)):\n",
    "            tot_corr+=1\n",
    "    print(\"Acc: {}\".format(tot_corr/tot_batches))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
